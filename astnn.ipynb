{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "astnn",
      "provenance": [],
      "authorship_tag": "ABX9TyOcupt/nE7V/eLop6EUlvc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SidrKhalid/MVC-Code-Smell-Detection/blob/main/astnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ASTNode(object):\n",
        "    def __init__(self, node):\n",
        "        self.node = node\n",
        "        # self.vocab = word_map\n",
        "        self.is_str = isinstance(self.node, str)\n",
        "        self.token = self.get_token()\n",
        "        # self.index = self.token_to_index(self.token)\n",
        "        self.children = self.add_children()\n",
        "\n",
        "    def is_leaf(self):\n",
        "        if self.is_str:\n",
        "            return True\n",
        "        return len(self.node.children()) == 0\n",
        "\n",
        "    def get_token(self, lower=True):\n",
        "        if self.is_str:\n",
        "            return self.node\n",
        "        name = self.node.__class__.__name__\n",
        "        token = name\n",
        "        is_name = False\n",
        "        if self.is_leaf():\n",
        "            attr_names = self.node.attr_names\n",
        "            if attr_names:\n",
        "                if 'names' in attr_names:\n",
        "                    token = self.node.names[0]\n",
        "                elif 'name' in attr_names:\n",
        "                    token = self.node.name\n",
        "                    is_name = True\n",
        "                else:\n",
        "                    token = self.node.value\n",
        "            else:\n",
        "                token = name\n",
        "        else:\n",
        "            if name == 'TypeDecl':\n",
        "                token = self.node.declname\n",
        "            if self.node.attr_names:\n",
        "                attr_names = self.node.attr_names\n",
        "                if 'op' in attr_names:\n",
        "                    if self.node.op[0] == 'p':\n",
        "                        token = self.node.op[1:]\n",
        "                    else:\n",
        "                        token = self.node.op\n",
        "        if token is None:\n",
        "            token = name\n",
        "        if lower and is_name:\n",
        "            token = token.lower()\n",
        "        return token\n",
        "\n",
        "    # def token_to_index(self, token):\n",
        "    #     self.index = self.vocab[token].index if token in self.vocab else MAX_TOKENS\n",
        "    #     return self.index\n",
        "\n",
        "    # def get_index(self):\n",
        "    #     return self.index\n",
        "\n",
        "    def add_children(self):\n",
        "        if self.is_str:\n",
        "            return []\n",
        "        children = self.node.children()\n",
        "        if self.token in ['FuncDef', 'If', 'While', 'DoWhile','Switch']:\n",
        "            return [ASTNode(children[0][1])]\n",
        "        elif self.token == 'For':\n",
        "            return [ASTNode(children[c][1]) for c in range(0, len(children)-1)]\n",
        "        else:\n",
        "            return [ASTNode(child) for _, child in children]\n",
        "\n",
        "    def children(self):\n",
        "        return self.children\n",
        "    #     if self.is_str:\n",
        "    #         return []\n",
        "    #     return [ASTNode(child) for _, child in self.node.children() if child.]\n",
        "\n",
        "\n",
        "class SingleNode(ASTNode):\n",
        "    def __init__(self, node):\n",
        "        self.node = node\n",
        "        self.is_str = isinstance(self.node, str)\n",
        "        self.token = self.get_token()\n",
        "        self.children = []\n",
        "\n",
        "    def is_leaf(self):\n",
        "        if self.is_str:\n",
        "            return True\n",
        "        return len(self.node.children()) == 0\n",
        "\n",
        "    def get_token(self, lower=True):\n",
        "        if self.is_str:\n",
        "            return self.node\n",
        "        name = self.node.__class__.__name__\n",
        "        token = name\n",
        "        is_name = False\n",
        "        if self.is_leaf():\n",
        "            attr_names = self.node.attr_names\n",
        "            if attr_names:\n",
        "                if 'names' in attr_names:\n",
        "                    token = self.node.names[0]\n",
        "                elif 'name' in attr_names:\n",
        "                    token = self.node.name\n",
        "                    is_name = True\n",
        "                else:\n",
        "                    token = self.node.value\n",
        "            else:\n",
        "                token = name\n",
        "        else:\n",
        "            if name == 'TypeDecl':\n",
        "                token = self.node.declname\n",
        "            if self.node.attr_names:\n",
        "                attr_names = self.node.attr_names\n",
        "                if 'op' in attr_names:\n",
        "                    if self.node.op[0] == 'p':\n",
        "                        token = self.node.op[1:]\n",
        "                    else:\n",
        "                        token = self.node.op\n",
        "        if token is None:\n",
        "            token = name\n",
        "        if lower and is_name:\n",
        "            token = token.lower()\n",
        "        return token\n"
      ],
      "metadata": {
        "id": "oyEysogMmnto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class BatchTreeEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, encode_dim, batch_size, use_gpu, pretrained_weight=None):\n",
        "        super(BatchTreeEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encode_dim = encode_dim\n",
        "        self.W_c = nn.Linear(embedding_dim, encode_dim)\n",
        "        self.W_l = nn.Linear(encode_dim, encode_dim)\n",
        "        self.W_r = nn.Linear(encode_dim, encode_dim)\n",
        "        self.activation = F.relu\n",
        "        self.stop = -1\n",
        "        self.batch_size = batch_size\n",
        "        self.use_gpu = use_gpu\n",
        "        self.node_list = []\n",
        "        self.th = torch.cuda if use_gpu else torch\n",
        "        self.batch_node = None\n",
        "        # pretrained  embedding\n",
        "        if pretrained_weight is not None:\n",
        "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
        "            # self.embedding.weight.requires_grad = False\n",
        "\n",
        "    def create_tensor(self, tensor):\n",
        "        if self.use_gpu:\n",
        "            return tensor.cuda()\n",
        "        return tensor\n",
        "\n",
        "    def traverse_mul(self, node, batch_index):\n",
        "        size = len(node)\n",
        "        if not size:\n",
        "            return None\n",
        "        batch_current = self.create_tensor(Variable(torch.zeros(size, self.encode_dim)))\n",
        "\n",
        "        index, children_index = [], []\n",
        "        current_node, children = [], []\n",
        "        for i in range(size):\n",
        "            if node[i][0] is not -1:\n",
        "                index.append(i)\n",
        "                current_node.append(node[i][0])\n",
        "                temp = node[i][1:]\n",
        "                c_num = len(temp)\n",
        "                for j in range(c_num):\n",
        "                    if temp[j][0] is not -1:\n",
        "                        if len(children_index) <= j:\n",
        "                            children_index.append([i])\n",
        "                            children.append([temp[j]])\n",
        "                        else:\n",
        "                            children_index[j].append(i)\n",
        "                            children[j].append(temp[j])\n",
        "            else:\n",
        "                batch_index[i] = -1\n",
        "\n",
        "        batch_current = self.W_c(batch_current.index_copy(0, Variable(self.th.LongTensor(index)),\n",
        "                                                          self.embedding(Variable(self.th.LongTensor(current_node)))))\n",
        "\n",
        "        for c in range(len(children)):\n",
        "            zeros = self.create_tensor(Variable(torch.zeros(size, self.encode_dim)))\n",
        "            batch_children_index = [batch_index[i] for i in children_index[c]]\n",
        "            tree = self.traverse_mul(children[c], batch_children_index)\n",
        "            if tree is not None:\n",
        "                batch_current += zeros.index_copy(0, Variable(self.th.LongTensor(children_index[c])), tree)\n",
        "        # batch_current = F.tanh(batch_current)\n",
        "        batch_index = [i for i in batch_index if i is not -1]\n",
        "        b_in = Variable(self.th.LongTensor(batch_index))\n",
        "        self.node_list.append(self.batch_node.index_copy(0, b_in, batch_current))\n",
        "        return batch_current\n",
        "\n",
        "    def forward(self, x, bs):\n",
        "        self.batch_size = bs\n",
        "        self.batch_node = self.create_tensor(Variable(torch.zeros(self.batch_size, self.encode_dim)))\n",
        "        self.node_list = []\n",
        "        self.traverse_mul(x, list(range(self.batch_size)))\n",
        "        self.node_list = torch.stack(self.node_list)\n",
        "        return torch.max(self.node_list, 0)[0]\n",
        "\n",
        "\n",
        "class BatchProgramClassifier(nn.Module):\n",
        "    # def __init__(self, embedding_dim, hidden_dim, vocab_size, encode_dim, label_size, batch_size, use_gpu=True, pretrained_weight=None):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, encode_dim, label_size, batch_size, use_gpu=True, pretrained_weight=None):\n",
        "        super(BatchProgramClassifier, self).__init__()\n",
        "        self.stop = [vocab_size-1]\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = 1\n",
        "        self.gpu = use_gpu\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encode_dim = encode_dim\n",
        "        self.label_size = label_size\n",
        "        #class \"BatchTreeEncoder\"\n",
        "        self.encoder = BatchTreeEncoder(self.vocab_size, self.embedding_dim, self.encode_dim,\n",
        "                                        self.batch_size, self.gpu, pretrained_weight)\n",
        "        self.root2label = nn.Linear(self.encode_dim, self.label_size)\n",
        "        # gru\n",
        "        self.bigru = nn.GRU(self.encode_dim, self.hidden_dim, num_layers=self.num_layers, bidirectional=True,\n",
        "                            batch_first=True)\n",
        "        # linear\n",
        "        self.hidden2label = nn.Linear(self.hidden_dim * 2, self.label_size)\n",
        "        # hidden\n",
        "        self.hidden = self.init_hidden()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        if self.gpu is True:\n",
        "            if isinstance(self.bigru, nn.LSTM):\n",
        "                h0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
        "                c0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
        "                return h0, c0\n",
        "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim)).cuda()\n",
        "        else:\n",
        "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim))\n",
        "\n",
        "    def get_zeros(self, num):\n",
        "        zeros = Variable(torch.zeros(num, self.encode_dim))\n",
        "        if self.gpu:\n",
        "            return zeros.cuda()\n",
        "        return zeros\n",
        "\n",
        "    def forward(self, x):\n",
        "        lens = [len(item) for item in x]\n",
        "        max_len = max(lens)\n",
        "\n",
        "        encodes = []\n",
        "        for i in range(self.batch_size):\n",
        "            for j in range(lens[i]):\n",
        "                encodes.append(x[i][j])\n",
        "\n",
        "        encodes = self.encoder(encodes, sum(lens))\n",
        "        seq, start, end = [], 0, 0\n",
        "        for i in range(self.batch_size):\n",
        "            end += lens[i]\n",
        "            if max_len-lens[i]:\n",
        "                seq.append(self.get_zeros(max_len-lens[i]))\n",
        "            seq.append(encodes[start:end])\n",
        "            start = end\n",
        "        encodes = torch.cat(seq)\n",
        "        encodes = encodes.view(self.batch_size, max_len, -1)\n",
        "\n",
        "        # gru\n",
        "        gru_out, hidden = self.bigru(encodes, self.hidden)\n",
        "\n",
        "        gru_out = torch.transpose(gru_out, 1, 2)\n",
        "        # pooling\n",
        "        gru_out = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)\n",
        "        # gru_out = gru_out[:,-1]\n",
        "\n",
        "        # linear\n",
        "        y = self.hidden2label(gru_out)\n",
        "        return y\n",
        "\n"
      ],
      "metadata": {
        "id": "naJ4ANMOmymy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycparser import c_parser, c_ast\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_sequences(node, sequence):\n",
        "    current = SingleNode(node)\n",
        "    sequence.append(current.get_token())\n",
        "    for _, child in node.children():\n",
        "        get_sequences(child, sequence)\n",
        "    if current.get_token().lower() == 'compound':\n",
        "        sequence.append('End')\n",
        "\n",
        "\n",
        "def get_blocks(node, block_seq):\n",
        "    children = node.children()\n",
        "    name = node.__class__.__name__\n",
        "    if name in ['FuncDef', 'If', 'For', 'While', 'DoWhile']:\n",
        "        block_seq.append(ASTNode(node))\n",
        "        if name is not 'For':\n",
        "            skip = 1\n",
        "        else:\n",
        "            skip = len(children) - 1\n",
        "\n",
        "        for i in range(skip, len(children)):\n",
        "            child = children[i][1]\n",
        "            if child.__class__.__name__ not in ['FuncDef', 'If', 'For', 'While', 'DoWhile', 'Compound']:\n",
        "                block_seq.append(ASTNode(child))\n",
        "            get_blocks(child, block_seq)\n",
        "    elif name is 'Compound':\n",
        "        block_seq.append(ASTNode(name))\n",
        "        for _, child in node.children():\n",
        "            if child.__class__.__name__ not in ['If', 'For', 'While', 'DoWhile']:\n",
        "                block_seq.append(ASTNode(child))\n",
        "            get_blocks(child, block_seq)\n",
        "        block_seq.append(ASTNode('End'))\n",
        "    else:\n",
        "        for _, child in node.children():\n",
        "            get_blocks(child, block_seq)\n",
        "\n"
      ],
      "metadata": {
        "id": "LAdQh6VFo_mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class Pipeline:\n",
        "    def __init__(self,  ratio, root):\n",
        "        self.ratio = ratio\n",
        "        self.root = root\n",
        "        self.sources = None\n",
        "        self.train_file_path = None\n",
        "        self.dev_file_path = None\n",
        "        self.test_file_path = None\n",
        "        self.size = None\n",
        "\n",
        "    # parse source code\n",
        "    def parse_source(self, output_file, option):\n",
        "        path = self.root+output_file\n",
        "        if os.path.exists(path) and option is 'existing':\n",
        "            source = pd.read_pickle(path)\n",
        "        else:\n",
        "            from pycparser import c_parser\n",
        "            parser = c_parser.CParser()\n",
        "            source = pd.read_pickle(self.root+'programs.pkl')\n",
        "\n",
        "            source.columns = ['id', 'code', 'label']\n",
        "            source['code'] = source['code'].apply(parser.parse)\n",
        "\n",
        "            source.to_pickle(path)\n",
        "        self.sources = source\n",
        "        return source\n",
        "\n",
        "    # split data for training, developing and testing\n",
        "    def split_data(self):\n",
        "        data = self.sources\n",
        "        data_num = len(data)\n",
        "        ratios = [int(r) for r in self.ratio.split(':')]\n",
        "        train_split = int(ratios[0]/sum(ratios)*data_num)\n",
        "        val_split = train_split + int(ratios[1]/sum(ratios)*data_num)\n",
        "        data = data.sample(frac=1, random_state=666)\n",
        "        train = data.iloc[:train_split] \n",
        "        dev = data.iloc[train_split:val_split] \n",
        "        test = data.iloc[val_split:] \n",
        "\n",
        "        def check_or_create(path):\n",
        "            if not os.path.exists(path):\n",
        "                os.mkdir(path)\n",
        "        train_path = self.root+'train/'\n",
        "        check_or_create(train_path)\n",
        "        self.train_file_path = train_path+'train_.pkl'\n",
        "        train.to_pickle(self.train_file_path)\n",
        "\n",
        "        dev_path = self.root+'dev/'\n",
        "        check_or_create(dev_path)\n",
        "        self.dev_file_path = dev_path+'dev_.pkl'\n",
        "        dev.to_pickle(self.dev_file_path)\n",
        "\n",
        "        test_path = self.root+'test/'\n",
        "        check_or_create(test_path)\n",
        "        self.test_file_path = test_path+'test_.pkl'\n",
        "        test.to_pickle(self.test_file_path)\n",
        "\n",
        "    # construct dictionary and train word embedding\n",
        "    def dictionary_and_embedding(self, input_file, size):\n",
        "        self.size = size\n",
        "        if not input_file:\n",
        "            input_file = self.train_file_path\n",
        "        trees = pd.read_pickle(input_file)\n",
        "        if not os.path.exists(self.root+'train/embedding'):\n",
        "            os.mkdir(self.root+'train/embedding')\n",
        "        from prepare_data import get_sequences\n",
        "\n",
        "        def trans_to_sequences(ast):\n",
        "            sequence = []\n",
        "            get_sequences(ast, sequence)\n",
        "            return sequence\n",
        "        corpus = trees['code'].apply(trans_to_sequences)\n",
        "        str_corpus = [' '.join(c) for c in corpus]\n",
        "        trees['code'] = pd.Series(str_corpus)\n",
        "        trees.to_csv(self.root+'train/programs_ns.tsv')\n",
        "\n",
        "        from gensim.models.word2vec import Word2Vec\n",
        "        w2v = Word2Vec(corpus, size=size, workers=16, sg=1, min_count=3)\n",
        "        w2v.save(self.root+'train/embedding/node_w2v_' + str(size))\n",
        "\n",
        "    # generate block sequences with index representations\n",
        "    def generate_block_seqs(self,data_path,part):\n",
        "        from prepare_data import get_blocks as func\n",
        "        from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "        word2vec = Word2Vec.load(self.root+'train/embedding/node_w2v_' + str(self.size)).wv\n",
        "        vocab = word2vec.vocab\n",
        "        max_token = word2vec.syn0.shape[0]\n",
        "\n",
        "        def tree_to_index(node):\n",
        "            token = node.token\n",
        "            result = [vocab[token].index if token in vocab else max_token]\n",
        "            children = node.children\n",
        "            for child in children:\n",
        "                result.append(tree_to_index(child))\n",
        "            return result\n",
        "\n",
        "        def trans2seq(r):\n",
        "            blocks = []\n",
        "            func(r, blocks)\n",
        "            tree = []\n",
        "            for b in blocks:\n",
        "                btree = tree_to_index(b)\n",
        "                tree.append(btree)\n",
        "            return tree\n",
        "        trees = pd.read_pickle(data_path)\n",
        "        trees['code'] = trees['code'].apply(trans2seq)\n",
        "        trees.to_pickle(self.root+part+'/blocks.pkl')\n",
        "\n",
        "    # run for processing data to train\n",
        "    def run(self):\n",
        "        print('parse source code...')\n",
        "        self.parse_source(output_file='ast.pkl',option='existing')\n",
        "        print('split data...')\n",
        "        self.split_data()\n",
        "        print('train word embedding...')\n",
        "        self.dictionary_and_embedding(None,128)\n",
        "        print('generate block sequences...')\n",
        "        self.generate_block_seqs(self.train_file_path, 'train')\n",
        "        self.generate_block_seqs(self.dev_file_path, 'dev')\n",
        "        self.generate_block_seqs(self.test_file_path, 'test')\n",
        "\n",
        "\n",
        "ppl = Pipeline('3:1:1', 'data/')\n",
        "ppl.run()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y4p7a2Dkmh5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwzJvvHKq9eW",
        "outputId": "71fc08c8-b605-4824-8414-c3d3759feb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from model import BatchProgramClassifier\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_batch(dataset, idx, bs):\n",
        "    tmp = dataset.iloc[idx: idx+bs]\n",
        "    data, labels = [], []\n",
        "    for _, item in tmp.iterrows():\n",
        "        data.append(item[1])\n",
        "        labels.append(item[2]-1)\n",
        "    return data, torch.LongTensor(labels)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    root = 'data/'\n",
        "    train_data = pd.read_pickle(root+'train/blocks.pkl')\n",
        "    val_data = pd.read_pickle(root + 'dev/blocks.pkl')\n",
        "    test_data = pd.read_pickle(root+'test/blocks.pkl')\n",
        "\n",
        "    word2vec = Word2Vec.load(root+\"train/embedding/node_w2v_128\").wv\n",
        "    embeddings = np.zeros((word2vec.syn0.shape[0] + 1, word2vec.syn0.shape[1]), dtype=\"float32\")\n",
        "    embeddings[:word2vec.syn0.shape[0]] = word2vec.syn0\n",
        "\n",
        "    HIDDEN_DIM = 100\n",
        "    ENCODE_DIM = 128\n",
        "    LABELS = 104\n",
        "    EPOCHS = 15\n",
        "    BATCH_SIZE = 64\n",
        "    USE_GPU = True\n",
        "    MAX_TOKENS = word2vec.syn0.shape[0]\n",
        "    EMBEDDING_DIM = word2vec.syn0.shape[1]\n",
        "\n",
        "    model = BatchProgramClassifier(EMBEDDING_DIM,HIDDEN_DIM,MAX_TOKENS+1,ENCODE_DIM,LABELS,BATCH_SIZE,\n",
        "                                   USE_GPU, embeddings)\n",
        "    if USE_GPU:\n",
        "        model.cuda()\n",
        "\n",
        "    parameters = model.parameters()\n",
        "    optimizer = torch.optim.Adamax(parameters)\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loss_ = []\n",
        "    val_loss_ = []\n",
        "    train_acc_ = []\n",
        "    val_acc_ = []\n",
        "    best_acc = 0.0\n",
        "    print('Start training...')\n",
        "    # training procedure\n",
        "    best_model = model\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        total_acc = 0.0\n",
        "        total_loss = 0.0\n",
        "        total = 0.0\n",
        "        i = 0\n",
        "        while i < len(train_data):\n",
        "            batch = get_batch(train_data, i, BATCH_SIZE)\n",
        "            i += BATCH_SIZE\n",
        "            train_inputs, train_labels = batch\n",
        "            if USE_GPU:\n",
        "                train_inputs, train_labels = train_inputs, train_labels.cuda()\n",
        "\n",
        "            model.zero_grad()\n",
        "            model.batch_size = len(train_labels)\n",
        "            model.hidden = model.init_hidden()\n",
        "            output = model(train_inputs)\n",
        "\n",
        "            loss = loss_function(output, Variable(train_labels))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # calc training acc\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total_acc += (predicted == train_labels).sum()\n",
        "            total += len(train_labels)\n",
        "            total_loss += loss.item()*len(train_inputs)\n",
        "\n",
        "        train_loss_.append(total_loss / total)\n",
        "        train_acc_.append(total_acc.item() / total)\n",
        "        # validation epoch\n",
        "        total_acc = 0.0\n",
        "        total_loss = 0.0\n",
        "        total = 0.0\n",
        "        i = 0\n",
        "        while i < len(val_data):\n",
        "            batch = get_batch(val_data, i, BATCH_SIZE)\n",
        "            i += BATCH_SIZE\n",
        "            val_inputs, val_labels = batch\n",
        "            if USE_GPU:\n",
        "                val_inputs, val_labels = val_inputs, val_labels.cuda()\n",
        "\n",
        "            model.batch_size = len(val_labels)\n",
        "            model.hidden = model.init_hidden()\n",
        "            output = model(val_inputs)\n",
        "\n",
        "            loss = loss_function(output, Variable(val_labels))\n",
        "\n",
        "            # calc valing acc\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total_acc += (predicted == val_labels).sum()\n",
        "            total += len(val_labels)\n",
        "            total_loss += loss.item()*len(val_inputs)\n",
        "        val_loss_.append(total_loss / total)\n",
        "        val_acc_.append(total_acc.item() / total)\n",
        "        end_time = time.time()\n",
        "        if total_acc/total > best_acc:\n",
        "            best_model = model\n",
        "        print('[Epoch: %3d/%3d] Training Loss: %.4f, Validation Loss: %.4f,'\n",
        "              ' Training Acc: %.3f, Validation Acc: %.3f, Time Cost: %.3f s'\n",
        "              % (epoch + 1, EPOCHS, train_loss_[epoch], val_loss_[epoch],\n",
        "                 train_acc_[epoch], val_acc_[epoch], end_time - start_time))\n",
        "\n",
        "    total_acc = 0.0\n",
        "    total_loss = 0.0\n",
        "    total = 0.0\n",
        "    i = 0\n",
        "    model = best_model\n",
        "    while i < len(test_data):\n",
        "        batch = get_batch(test_data, i, BATCH_SIZE)\n",
        "        i += BATCH_SIZE\n",
        "        test_inputs, test_labels = batch\n",
        "        if USE_GPU:\n",
        "            test_inputs, test_labels = test_inputs, test_labels.cuda()\n",
        "\n",
        "        model.batch_size = len(test_labels)\n",
        "        model.hidden = model.init_hidden()\n",
        "        output = model(test_inputs)\n",
        "\n",
        "        loss = loss_function(output, Variable(test_labels))\n",
        "\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_acc += (predicted == test_labels).sum()\n",
        "        total += len(test_labels)\n",
        "        total_loss += loss.item() * len(test_inputs)\n",
        "    print(\"Testing results(Acc):\", total_acc.item() / total)\n"
      ],
      "metadata": {
        "id": "ph3QC-Rxqag5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}